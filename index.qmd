---
title: "Weekly Summary Template"
author: "Advait Ashtikar"
title-block-banner: true
title-block-style: default
toc: true
# format: html
format: pdf
---

------------------------------------------------------------------------

## Tuesday, Mar 14

::: callout-important
## TIL

Include a *very brief* summary of what you learnt in this class here.

Today, I learnt the following concepts in class:

1.  $k$-Fold Cross Validation
:::

### Libraries Needed

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(caret)
library(glmnet)
library(ISLR2)
library(readr)
library(purrr)
library(torch)
library(mlbench)
```

### $k$-Fold Cross Validation

We can reduce the variability in `mspe` by using $k$-fold method. We get $k$ mspe's for all the various models, this can be averaged to get a stable model.

```{r}
attach(Boston)

df <- Boston %>%
  drop_na()

head(df)
```

```{r}
k <- 5
folds <- sample(1:k, nrow(df), replace = T)
folds

df_folds <- list()

for(i in 1:k){
  df_folds[[i]] <- list()
  df_folds[[i]]$tain = df[which(folds != i), ]
  df_folds[[i]]$test = df[which(folds == 1), ]
}
```

```{r}
nrow(df_folds[[2]]$train) + nrow(df_folds[[2]]$test) - nrow(df)
```

```{r}
nrow(df_folds[[3]]$train) + nrow(df_folds[[4]]$test) - nrow(df)
```

> The value for the training and testing dataset (the fold chosen) has to be the same to get consistent values.

```{r}
kfold_mspe <- c()
for(i in 1:k){
  model <- lm(medv ~ ., df_folds[[i]]$train)
  y_hat <- predict(model, df_folds[[i]]$test)
  kfold_mspe[i] <- mean((y_hat - df_folds[[i]]$test$medv)^2)
}

kfold_mspe
```

### Wrapped in a function

```{r}
make_folds <- function(df, k){
  folds <- sample(1:k, nrow(df), replace = T)
  df_folds <- list()
  for(i in 1:k){
    df_folds[[i]] <- list()
    df_folds[[i]]$train <- df[which(folds != 1), ]
    df_folds[[i]]$test <- df[which(folds == 1), ]
  }
  return(df_folds)
}
```

```{r}
cv_mspe <- function(formula, df_folds){
  kfold_mspe <- c()
  for(i in 1:length(df_folds)){
    model <- lm(formula, df_folds[[i]]$train)
    y_hat <- predict(model, df_folds[[i]]$test)
    kfold_mspe[i] <- mean((y_hat - df_folds[[i]]$test$medv)^2)
  }
  return(mean(kfold_mspe))
}
```

```{r}
cv_mspe(medv ~ ., make_folds(df, 5))
```

```{r}
cv_mspe(medv ~ 1, df_folds)
```

### Using the `caret` package

We define the training control for cross validation

```{r}
ctrl <- trainControl(method = "cv", number = 5)
```

```{r}
model <- train(medv ~ ., data = df, method = "lm", trControl = ctrl)
summary(model)
```

```{r}
predictions <- predict(model, df)
```

### `caret` for LASSO

Bias-variance tradeoff

```{r}
ctrl <- trainControl(method = "cv", number = 5)

#Defining the tuning grid
grid <- expand.grid(alpha = 1, lambda = seq(0, 0.1, by = 0.001))

#Train the model using lasso regression with cross-validation
lasso_fit <- train(
  medv ~ .,
  data = df,
  method = "glmnet",
  trControl = ctrl,
  tuneGrid = grid,
  standardize = TRUE,
  family = "gaussian"
)

plot(lasso_fit)
```

> By changing our grid search space, we can see the optimum values. We are also able to see how fine our cross-validation is.

## Thursday, Mar 16

::: callout-important
## TIL

Include a *very brief* summary of what you learnt in this class here.

Today, I learnt the following concepts in class:

1.  Classification
2.  Logistic Regression
3.  Logistic Regression with `torch`
4.  Optimization
5.  Logistic Loss Function
:::

$$
\boxed{y = \beta_0 + \beta_1 x_1 + \dots \beta_p x_p}
$$

looking at different loss functions:

1.  Least squares:

$$
L(\beta) = \sum_{i-1}^n \| y_i - \beta_0 - \beta_1 x_1 - \dots - \beta_p x_p \|^2
$$

2.  Penalized least squares/LASSO:

$$
L(\beta) = \sum_{i-1}^n \| y_i - \beta_0 - \beta_1 x_1 - \dots - \beta_p x_p\|^2 + \lambda \|{\beta}\|_1
$$

### Classification

We will be using the following dataset for the example here

> Breast cancer dataset: This dataset contains measurements of various characteristics of breast cancer cells, with the goal of predicting whether a tumor is benign or malignant.

```{r}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

col_names <- c("id", "diagnosis", paste0("feat", 1:30))
df <- read_csv(
  url, col_names, col_types = cols()) %>%
  select(-id) %>%
  mutate(outcome = ifelse(diagnosis == "M", 1, 0)) %>%
  select(-diagnosis)

head(df)
```

The problem with linear regression for binary responses.

Let's start by looking at an example. Imagine we have a dataset with a binary response variable (0 or 1) and a continuous predictor variable. We might be tempted to use linear regression with the `lm()` function to model the relationship between the predictor and response variables. After all, linear regression is a powerful and flexible tool that can be used to model a wide range of relationships between variables.

However, when we use linear regression with a binary response variable, we quickly run into a problem. The linear regression model will give us a predicted value for the response variable for any given value of the predictor variable, but this predicted value is not a probability. The predicted value can take on any value between 0 and 1, but it doesn't necessarily represent the probability of the response variable.

If we interpret the predicted value as a probability, we might conclude that the probability of the response variable being a 1 when the predictor variable has a value of 1.5 is 0.8. But this interpretation is incorrect. The predicted value from linear regression is not a probability, and it can take on values greater than 1 or less than 0.

```{r}
reg_model <- lm(outcome ~ ., df)
summary(reg_model)
```

```{r}
n <- 100
new_patients <- data.frame(matrix(rnorm(30 * n), nrow = n))
colnames(new_patients) <- paste0("feat", 1:30)
new_predictions <- predict(reg_model, newdata = new_patients, type = "response")
```

```{r}
print(new_predictions %>% head())
```

```{r}
boxplot(new_predictions)
```

### The need for Logistic Regression

So, what do we do when we have a binary response variable and we want to model the relationship between the predictor and response variable? This is where **logistic regression** comes in. Logistic regression is a type of generalized linear model that is specifically designed for binary response variables.

The main idea behind logistic regression is to transform the predicted values from linear regression so that they represent probabilities. We do this using a function called the logistic function, which maps any value between negative infinity and positive infinity to a value between 0 and 1. The logistic function is a sigmoidal curve that looks like an elongated S-shape. By applying the logistic function to the predicted values from linear regression, we can transform them into probabilities that represent the probability of the response variable being a 1.

In the next section, we'll dive into the details of logistic regression and see how it works in practice using the breast cancer dataset.

### Odds and odds ratios

Let's start by defining the odds of an event occurring. The odds of an event occurring are defined as the probability of the event occurring divided by the probability of the event not occurring. For example, if the probability of a basketball team winning a game is 0.6, then the odds of the team winning the game are 0.6/0.4 = 1.5.

Odds ratio are a way to compare the odds of an event occurring between two different groups. The odds ratio is defined as the ratio of the odds of an event occurring in one group to the odds of the event occurring in another group. For example, if the odds of. basketball winning a game are 1.5 in one group and 2.0 n another group, then the odds ratio of the first group to the second group is 1.5/2.0 = 0.75.

```{r}
set.seed(123)
binary_var <- rbinom(100, size = 1, prob = 0.6)
group_var <- sample(1:2, size = 100, replace = TRUE)
odds_group1 <- sum(binary_var[group_var == 1]) / sum(!binary_var[group_var == 1])
odds_group2 <- sum(binary_var[group_var == 2]) / sum(!binary_var[group_var == 2])
odds_ratio <- odds_group1 / odds_group2
cat(paste("Odds group 1:", round(odds_group1, 2), "\n"))
cat(paste("Odds group 2:", round(odds_group2, 2), "\n"))
cat(paste("Odds ratio:", round(odds_ratio, 2), "\n"))
```

### Logistic Regression Model

Now let's move on to the logistic regression model. The logistic regression model is a type of generalized linear model that models the probability of an event occurring as a function of one or more predictor variables. The logistic regression model uses the logistic function, also known as the sigmoid function, to model the relationship between the predictor variables and the probability of the event occurring.

**The sigmoid function is given as follows:**

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

```{r}
sigmoid <- \(x) 1 / (1 + exp(-x))

curve(sigmoid, -7, 7, ylab = "sigmoid(x)")
```

In logistic regression, the underlying model is assumed to be of the form

$$
\boxed{
p(x) = \sigma(\beta_0 + \beta_1 x) = \frac{1}{1 + exp(-\beta_0 - \beta_1 x)}
}
$$

where $p(x)$ is the probability of the event occurring given the value of predictor variable $x$, and b0 and b1 are the **intercept** and **slope** coefficients of the logistic regression model, respectively.

> $p(x)$ is guaranteed to be a probability for all values of $x$.

Notice how this is similar to **linear regression** which has

$$
y(x) = \beta_0 + \beta_1 x
$$

The logistic regression function has an S-shaped curve and maps any real-valued input to a probability between 0 and 1. As such, the logistic regression model is well-suited for modeling binary response variables, where the goal is to predict probability of an event occurring (e.g., whether a customer will buy a product or not).

#### Logistic Regression Example

The `glm()` function fits a generalized linear model, which includes logistic regression as a special case.

```{r}
set.seed(123)
x <- rnorm(100)
y <- rbinom(100, size = 1, prob = exp(0.5 + 0.8 * x)/(1 + exp(0.5 + 0.8 * x)))
```

```{r}
model <- glm(y ~ x, family = binomial())
summary(model)
```

```{r}
x_test <- -0.5
sigmoid(coef(model)[1] + coef(model)[2] * x_test)
```

```{r}
predict(model, newdata = data.frame(x = x_test, type = "response"))
```

```{r}
new_x <- seq(-2, 2, by = 0.1)
p1 <- predict(model, data.frame(x = new_x))
p2 <- predict(model, data.frame(x = new_x), type = "response")
```

```{r}
boxplot(p1, p2)
```

#### Logistic Regression for Breast Cancer

Let's start by fitting a logistic regression model to the breast cancer dataset using the `glm()` function in R.

```{r}
df <-
  df %>%
  mutate_at("outcome", factor)
```

```{r}
model <- glm(outcome ~ ., data = df, family = binomial())
summary(model)
```

> The output of the summary() function provides a summary of the model, including the coefficients of each predictor, their standard errors, and the corresponding p-values. The coefficients represent the log odds ratio of the response variable for each predictor. We can exponentiate the coefficients to get the odds ratio.

```{r}
new_patient <- data.frame(matrix(rnorm(30), nrow = 1))
names(new_patient) <- paste0("feat", 1:30)
predict(model, newdata = new_patient, type = "response")
```

### Logistic Regression with `torch` library

Now that we have the `torch` library installed, we can perform logistic regression using the following steps:

1.  Convert the data to a tensor
2.  Define the model architecture
3.  Define the loss function
4.  Define the optimizer
5.  Train the model
6.  Make predictions

```{r}
X <- cbind(x)
x_tensor <- torch_tensor(X, dtype = torch_float())
y_tensor <- torch_tensor(y, dtype = torch_float())
```

```{r}
module <- nn_module(
  "logistic regression",
  initialize = function() {
    self$fc1 <- nn_linear(1, 1)
    self$fc2 <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$fc1() %>%
      self$fc2()
  }
)
```

```{r}
logistic_reg <- module()
```

```{r}
y_pred <- logistic_reg(x_tensor)
y_pred %>% head()
```

#### Question: What is an appropriate loss function?

```{r}
L <- function(x, y, model){
  y_pred <- model(x)
  return (mean((y_pred - y)^2))
}
```

```{r}
logistic_reg_1 <- module()
L(x_tensor, y_tensor, logistic_reg)
```

### Optimization

```{r}
optimizer <- optim_adam(logistic_reg_1$parameters, lr = 0.0001)

epochs <- 10000
for(i in 1:epochs){
  loss <- L(x_tensor, y_tensor, logistic_reg_1)
  
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  if(i %% 1000 == 0){
    cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
  }
}
```

### Logistic Loss Function

#### a.k.a Binary Cross Entropy `nn_bce()`

```{r}
nn_bce_loss()
```

```{r}
L2 <- function(x, y, model){
  nn_bce_loss()(model(x), y)
}
```
